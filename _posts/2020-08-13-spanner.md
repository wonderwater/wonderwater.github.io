---
layout: post
title: "Spanner"
date: 2020-08-13 21:58
categories: "6.824"
tags:
    - distribution
    - database
---

Google Spanner, OSDI 2012

为什么读这篇文章(Google Spanner, OSDI 2012)？
1. 少见的大规模应用的分布式事务案例
    - 非常棒
    - 但是两阶段提交被认为太慢并且易于阻塞
2. 少见的大规模应用的同步复制的案例
    - 基于Paxos的两阶段提交
    - 快速r/o事务的时间同步
3. 大量用于Google内部

用例？
Google F1推荐的数据库（Sec5.4）
先前通过许多MySQL和BigTable数据库进行分片；尴尬。
需要：
- 更好（同步）复制
- 更灵活的分片
- 跨分片事务

工作负载由只读事务控制（Table 6）
要求强一致性：外部一致性、可线性化、可串行化

基本组织：
```text
  Datacenter A:
    "clients" are web servers e.g. for gmail
    data is sharded over multiple servers:
      a-m
      n-z
  Datacenter B:
    has its own local clients
    and its own copy of the data shards
      a-m
      n-z
  Datacenter C:
    same setup
```

复制管理基于Paxos：每个分片都是一个Paxos组
副本在不同的数据中心，
和Raft类似，每个Paxos组都有leader，
在实验里，Paxos复制log的操作

为什么这么安排？
- 分片可以指支持并发，提高总的吞吐量
- 支持数据独立的失败——处于不同的城市
- client可以读本地副本——快！
- 可以将副本放到离客户近的地方
- Paxos要求只要大多数——容忍慢/远的副本

有哪些挑战？
- 读取本地的副本要是新数据，但本地的副本也许不会反应最新的Paxos的写入
- 事务要协调多个分片，即多个Paxos组
- 事务读取多记录，必须可串行化，
但是本地分片可能反映了已提交事务的不同子集！

Spanner对待读/写和只读事务是不一样的。

## 读/写事务
例子：银行转账
```text
  BEGIN
    x = x + 1
    y = y - 1
  END
```
我们不想读或写x或y的中间状态。提交后，所有的读要能看到我们的更新

总结：基于Paxos复制参与者的两阶段提交
（r/w事务）
client挑一个TID
client每次发生读请求给分片的Paxos组的leader（2.1）
    - 每个分片首先请求相关记录的锁（可能要等待）
    - 每个分片有独立的记录锁的表（简称锁的表），在分片leader中
    - 读锁不会通过Paxos复制，所以leader失败了，就是中止
client保持私有的写入直到提交
当client提交（4.2.1）
    - 选择一个Paxos组做为2pc的事务协调者（TC）
    - 发送写给相关分片的leaders
    - 每个被写的分片leader：
        - 请求写记录的锁
        - 通过Paxos记录Prepare的log（包含锁和新值）
        - 回复TC已经prepared
        - 或者回复TC“no”如果挂了因此丢失了锁的表.
    - TC:
        - 决定commit或abort
        - 通过Paxos发决定结果的logs
        - 告诉参与者的leader和client结果
    - 每个分片的leader：
        - 通过Paxos发TC的决定结果的log
        - 释放表锁

设计的一些点：
1. 两阶段的锁要确保可串行化
2. 2pc一般的b/c问题，当TC挂了，导致阻塞和锁不释放，通过Paxos复制TC解决了问题
3. r/w事务耗时长
    - 许多数据中心间的消息
    - Table6描述跨美国的r/w事务大约100ms
    - 比跨城市的要小很多（table3）
4. 许多并行性：多client，多分片，所以总的吞吐量更高

到现在我们一致认为每个Paxos组是一个单个实体
- 复制分片数据
- 复制两阶段提交的状态

对于只读（read-only，r/o）事务
要解决多分片上的多读的问题，我们希望r/o的xactions比r/w更快

Spanner消除了r/o事务的两大消耗
从本地副本读，避免Paxos和跨数据中心的通信，但是本地副本不一定是最新的！
无锁，没有两阶段提交，没有事务管理
    - 再一次避免跨数据中心的通信
    - 避免慢的r/w事务
Table3、6表明延迟降低了10倍，那如何解决正确性？

## r/w事务的正确性约束：
可串行化：
结果要和事务一个一个执行一样，即使事务真的并发执行。
比如只读xactions基本在r/w的xaction之间
可以看到之前事务的写，而不是后续事务的

外部约束：
如果T1在T2开始前完成，T2要能看到T1的写，“前”意味着real time，和可线性化类似，拒绝读取旧数据

加上两个银行间转账
```text
    T1:  Wx  Wy  C
    T2:                 Wx  Wy  C
    T3:             Rx             Ry
```
结果不能满足任何可串行的顺序，比如
不是T1, T2, T3.
不是T1, T3, T2.
我们想要T3看到T2的执行结果，或者完全看不到
我们想要T3的读，相对于T1/T2处于同一点的结果


想法：快照隔离（Snapshot Isolation, SI）
- 同步所有计算机时钟（真实的钟）
- 给每个事务一个时间戳
    - r/w: 提交时间
    - r/o: 开始时间
- 执行时好像时间戳顺序一次一条
    - 计算真的读是不同顺序发生的
- 每个副本存储每条记录的多个时间戳版本
    - 所有的r/w事务的写获得同样的时间戳
- w/o事务的读看到xactions的时间戳
    - 记录的最高时间戳版本小于xaction的时间戳
- 称为快照隔离

我们的例子中：
```text
                      x@10=9         x@20=8
                      y@10=11        y@20=12
    T1 @ 10:  Wx  Wy  C
    T2 @ 20:                 Wx  Wy  C
    T3 @ 15:             Rx             Ry
```

@10 表示时间戳
T3读的都是@10的版本，T3不能看到T2，及时T3的读y发生在T2以后。
现在的结果是可串行化的：T1 T2 T3，串行化的顺序和时间戳时候一致的

为什么T3有更新的值，却读老值，是ok的？
T2、T3是并发的，所以外部一致性要求任意顺序，记住，只读事务需要读取他们时间戳的值，而不是最新的写入

问题：要是T3从副本读取的x，并不是T1写入的？因为这个副本没有在Paxos达到大多数。

解决：副本安全时间（safe time）
Paxos的leader按时间戳顺序发送写
在处理读时间戳20这以前，副本能看到Paxos的写的时间戳已经大于20，所以小于20的写都能看到
如果有prepared但未提交的事务，必须延迟（Sec4.1.3）
因此：只读事务可以从本地的副本读——通常更快

问题：要是时钟并没有完美的同步？

要是时钟没有精确同步会发生什么？
- 对r/w事务没问题，因为持有锁
- 如果只读事务的时间戳太大，比副本的安全时间都大，那么读将阻塞——正确但是慢，差越多则越慢
- 如果事务时间戳太小，会错过只读事务开始前的写提交，因为更低的时间戳会读到旧版本的数据，违反了外部一致性


只读事务xaction时间戳太小的例子：
```text
  r/w T0 @  0: Wx1 C
  r/w T1 @ 10:         Wx2 C
  r/o T2 @  5:                   Rx?
(C for commit)
```
这会造成T2读到@0版本的x，即读到1，但是T2真正是在T1提交后开始的，所以外部一致性要求T2读到x=2，
所以要解决错误时钟的可能。

我们可以完美的同步计算机的时钟吗？
每个数据中心的每台计算机，都是相同的时间，比如，全都是2:00pm on Tuesday April 7 2020

在实践中，并非完美的。

时间由一系列组织定义，使用不同协议分发，比如GPS, WWV, NTP
分布式中的延迟总有波动，很难预测，所以总是不确定的，加上分布式系统存在意外故障的情况

Google的时间参照系统（Sec5.3）

  [UTC, GPS satellites, masters, servers, TTinterval]

- 每个数据中心有一些time master服务器
- 每个time master接收GPS或原子钟
- GPS接收器通常精确到一微秒以上。
- 论文没有说明为什么用原子钟。
    - 可能同步了GPS，一段时间没同步依然精确
    - 如果久了，错误会累积，每周可能有微秒的误差
- 其他服务请求最近的time masters
    - 由于网络延迟的不确定性，每次访问间存在漂移。

TrueTime
时间服务有TTinterval = [ earliest, latest ]，
正确的而时间保证在这个区间中，
区间的宽度来源于网络延迟的测量，时钟硬件规范，
Figure6：区间通常是微秒级的，但有时10+ms，所以服务器时钟并不能精确同步，
但是TrueTime提供了服务器时钟的误差范围。

Spanner如何保证r/w T1的完成早于r/o T2的开始，TS1< TS2
即r/o事务时间戳不会太小。

两条规则：

规则一：开始规则：
xaction TS = TT.now().latest
- r/o线程，开始时间
- r/w线程，开始提交时间

规则二：提交等待，对于r/w xaction：
- 提交前，延迟，直到TS < Ts.now().earliest
- 保证TS已经过去了

案例：区间和提交等待

T1提交，然后T2开始，T2必须要看到T1的写入，即TS1 < TS2
```text
  r/w T0 @  0: Wx1 C
                   |1-----------10| |11--------------20|
  r/w T1 @ 10:         Wx2 P           C
                                 |10--------12|
  r/o T2 @ 12:                           Rx?
(P for prepare)
```
C保证发生在TS(10)之后，因为提交等待规则。
假设Rx发生在在C之后，因此是10之后。
T2选择TT.now().latest，比当前时间大，即大于10，所以TS2 > TS1。

## 为什么可以支持外部一致性？
提交等待意味着r/w的TS保证已经过去了，
r/o TS=TT.now().latest保证大于正确的时间，
因此也大于TS之前已提交的所有事务

更一般的：
快照隔离使得r/o事务可串行化。
- 时间戳当做顺序
- 快照版本（和安全时间）实现时间戳的一致性读
- Xaction可以看到所有低于TS的xactions，而看不到高于的
- 任何数字都能做TS，如果不关系外部一致性

同步时间戳提供外部一致性
- 即使是夸数据中心的事务
- 即使读本地可能延迟的副本

为什么这些都很有用？

更快的只读事务：
- 在client的数据中心读副本
- 无锁，不用两阶段提交
- 因此延迟降低了10倍（Table3、6）

但是
- 只读事务的读可能由于安全时间阻塞（进行同步数据）
- r/w事务提交阻塞（因为提交等待）
- 精确（小区间）的时间最小化以上的阻塞延迟

总结：

- 很少能看到生产的系统提供分布式事务，提别说上G的分布式数据
- Spanner是实践里的，令人惊喜的范例
- 时间戳模式是最热门的领域
- 在Google大规模应用，Google的商业服务，影响力大


















































