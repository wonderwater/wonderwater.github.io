---
layout: post
title: "6.824lab3解题"
date: 2020-08-01 09:30
categories: "6.824"
tags:
    - distribution
    - algorithm
---

Lab3的实验主题是基于Raft协议的k/v服务的实现，并且继续Raft论文后半部分关于Snapshot的实现。 
6.824-2020 ， 
Lab3A: Key/value service without log compaction ， 
Lab3B: Key/value service with log compaction。

## 前言

Lab3的实验主题是基于Raft协议的k/v服务的实现，并且继续Raft论文后半部分关于Snapshot的实现。

Lab2扎实的完成后，Lab3相对Lab2要简单，参考课程小结、实验指导书。

本文打算把实验的设计用语言描述，以及总结一些注意事项。

先上实验结果，欢迎讨论。

```text
Test: one client (3A) ...
  ... Passed --  15.1  5  7748  718
Test: many clients (3A) ...
  ... Passed --  15.4  5 24607 3224
Test: unreliable net, many clients (3A) ...
  ... Passed --  16.1  5  8420 1260
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   0.9  3   214   52
Test: progress in majority (3A) ...
  ... Passed --   0.3  5    42    2
Test: no progress in minority (3A) ...
  ... Passed --   1.1  5   121    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    67    3
Test: partitions, one client (3A) ...
  ... Passed --  22.5  5 12433  510
Test: partitions, many clients (3A) ...
  ... Passed --  23.0  5 65133 2335
Test: restarts, one client (3A) ...
  ... Passed --  19.5  5 15759  713
Test: restarts, many clients (3A) ...
  ... Passed --  19.0  5 55188 3159
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  20.5  5  9087 1260
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  26.4  5 48869 1785
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  27.5  5  7009  698
Test: unreliable net, restarts, partitions, many clients, linearizability checks (3A) ...
  ... Passed --  25.3  7 17666 1276
Test: InstallSnapshot RPC (3B) ...
  ... Passed --   4.2  3  3291   63
Test: snapshot size is reasonable (3B) ...
  ... Passed --  17.3  3  5666  800
Test: restarts, snapshots, one client (3B) ...
  ... Passed --  19.8  5 19744  708
Test: restarts, snapshots, many clients (3B) ...
  ... Passed --  20.0  5 92609 10392
Test: unreliable net, snapshots, many clients (3B) ...
  ... Passed --  16.2  5  8592 1270
Test: unreliable net, restarts, snapshots, many clients (3B) ...
  ... Passed --  19.8  5  8894 1247
Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ...
  ... Passed --  27.8  5  7799  899
Test: unreliable net, restarts, partitions, snapshots, many clients, linearizability checks (3B) ...
  ... Passed --  25.3  7 20073 1563
PASS
ok  	6.824/src/kvraft	384.614s

```

## 代码设计

### k/v Service设计：

参考课程小结，[duplicate RPC detection][1]

client开始执行指令（Get/PutAppend），将指令发送给server，直到成功收到server的返回。
这么做的目的是，一个client一次只能发一个指令。

如果client未收到响应，将会重发指令，但这个未收到，有两种情况
- 网络丢包，请求未送达server
- server已处理，网络丢包，响应未送达client

这两种情况客户端是识别不出来的，所以请求报文加了clientID、seq#，标识请求的唯一性，
将这次处理的结果放到map中，下次server在收到同样的报文，根据clientID和seq#，从map拿取数据，
返回即可，但如果每个请求都放到map，map会越来越大，所以设计这个map的key是clientID，
并且要求client发送指令的seq#是递增的，一条指令完成后，才可以发下一次指令，所以当一个seq#到达server时，
可以保证seq#之前的请求已经都完成了。

kvServer从applyCh获取提交的log时，不仅要维护自己的状态机数据，还要维护防重的map，
所以在logEntry中也加上clientID和seq#。这样的话，每个节点维护了一个相同的防重的map。
另外，和kvServer自身的状态机数据一样，Snapshot时也要将防重的map进行存储。

server对Get/PutAppend请求的的处理其实是类似的
- 将指令交给Raft
- 等待指令被Raft发送到applyCh
    - `kv.cond.Wait()`实现等待；`kv.cond.Broadcast()`由ApplyLoop通知并唤醒
    - 等待的过程可能Raft的状态已经发生改变，
    在我的实现里，通过后台任务，每隔1s，唤醒等待goroutine检查是否需要返回失败，进而client重发
- 从防重map获取结果，返回成功

### Snapshot设计：

```text
rf.log数组的结构
index  0                                   1
      |lastIncludedIndex/lastIncludedTerm |截下的log ......
```

`log[0]`被用作存储Snapshot的最后一个log，所以做Snapshot前的index，映射到做了Snapshot后的log数组的索引就是index-lastIncludedIndex。
然后要对心跳处理的日志同步的地方做对应的处理。

在Lab3中，InstallSnapshot的数据一次性发送即可，不用文件存储。回顾步骤：
- 如果args.Term小于currentTerm，直接返回
- 判断LastIncludedIndex，检查是否值得install？
    - 回顾这个RPC最主要的目的是什么？
    - 在leader的log中，LastIncludedIndex及之前的log已经没了，所以才要用这个RPC同步状态机数据
    - 所以目的是同步状态机数据，那问题就是什么样的状态机数据值得install？
    - lastApplied比LastIncludedIndex小的情况
    - 另外，commitIndex大于等于LastIncludedIndex，也代表节点有能力恢复日志
- 截断log数据（索引0的位置要保证是lastIncludedIndex的数据），更新Raft的lastApplied、lastIncludedIndex、commitIndex（如有必要），并持久化
- 最后将snapshot包装成ApplyMsg通过applyCh发给kvServer。为了保证kvServer收到snapshot前，
已经更新的lastApplied不会提前被kvServer应用，所以，锁不能提前释放。

## 注意事项

- kv.mu和rf.mu不要嵌套着锁。
- 建议对每个if语句都排查一下，是否需要再考虑else的情况。
- kvServer的数据序列化，encode错误
    - StartKVServer函数开头部分已经说明了
    - 加代码`labgob.Register(map[string]string{})`
- 优雅的关闭
    - 有些任务判断Killed()，即可退出，比如超时选举任务
    - kvServer的ApplyLoop，即使关闭时，依然卡在applyCh的数据接收的位置
        - 给applyCh发特殊的ApplyMsg，代表关闭。谁发这个ApplyMsg？
        - 现在的ApplyMsg都是谁发的？Raft的ApplyLoop。
        - applyCh的发送和接收都会hold住，所以发送和接收要配对，
        才不会导致另一方卡住，所以也要由Raft的ApplyLoop发关闭的ApplyMsg
- 启动Raft，保证一定顺序
    - 首先snapshot要发给kvServer，应用成功后，在做之后的启动（这一步可以写在一个goroutine，防止kvServer的启动被阻塞）
    - 剩下的log，通过后台任务，发给kvServer



[1]: https://pdos.csail.mit.edu/6.824/notes/l-raft2.txt















