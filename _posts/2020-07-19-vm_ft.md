---
layout: post
title: "VM-FT论文阅读"
date: 2020-07-19 17:00
categories:
    - 6.824
tags:
    - paper
    - fault_tolerance
    - distribution
---

主备复制，本文翻译6.824的Lecture 4

## 主题
1. 通过主备复制研究容错
2. VMware FT论文，一个极端的想法

通过复制，即使服务、网络发生故障，也能提供可用性

## 什么样的故障可以通过复制解决？
- 单副本的"fail-stop"失败
    1. 主板风扇坏了、CPU过热导致停机
    2. 副本的电源线或网线被打翻
    3. 磁盘空间不足被软件检测到，停机
- 不是软硬件、软件bug、手工配置的错误
    1. 这些问题不常是fail=stop的
    2. 出现问题的关联度高（有问题一般同时出现）
    3. 有些能检测到（比如硬盘checksums，可能发现磁盘错误）
- 地震、城市停电？

副本间要做物理上的隔离

## 值得为复制消耗N倍的成本吗？

## 两种主要的复制方案
1. 状态转移（State transfer）
    primary执行服务
    primary把状态发给各个备份（backups）

2. 复制状态机（Replicated state machine）
    clients发给primary操作，primary给操作排序，再发送给备份，
    所有副本执行所有操作，
    如果副本初始的状态一直，那么，相同的操作，相同的顺序，最后，也应该是相同的状态
    
## 状态转移更简单
    但是需要通过网络发更大的数据，因此也更慢

## 复制状态机的网络负载更低
1. 只发操作，比发状态的数据，小得多
2. 实现复杂
3. VM-FT用的是状态转移，Labs 2/3/4

## 主要问题
1. 哪些状态是需要复制的？
2. primary需要等备份完成吗？
3. 何时切到副本
4. 切换过程中会发生什么问题？
5. 副本如何加快备份的处理

## 我们希望副本在哪一层次是相同的
1. 应用状态，比如数据库表
- GFS
- 更高效，primary只要发高层级的操作给副本
- 应用需要做容错，比如转发操作流给副本
2. 机器级，比如寄存器、内存的内容
- 或许可以实现服务器上所有应用的复制
- 要求转发机器事件（中断、DMA、部分指令（比如随机数））
- 需要修改"机器"，以完成事件流的发送和接受，比如定制CPU

## VM-FT用的机器级的状态复制
1. 可移植性：在用在现有的OS和服务器的应用
2. 对clients表选为单机一样

## 论文概述

![主备基本结构](/pic/202007/2020-07-19-16.14.14.png)

- 词汇：
    hypervisor == monitor == VMM (virtual machine monitor)，
    O/S+app作为"guest"运行在VM上
- 两台机器：primary、backup
- primary 发送所有外部事件，包成log entries，通过网络（logging channel）给到backup
- backup的输出受到FT的监视
- 如果任何一台，通过网络联系不到对方，
"goes live"，提供单独的服务。如果primary goes live，就不发log entries给backup了

## VM模拟了本地磁盘接口，但用的是网络存储服务
    备份接管是不用复制磁盘了，更快
    backup goes live，需要访问disk server（防止发生脑裂）

## primary何时发消息给backup
1. 任何导致执行不一样的时候
2. 非确定的指令序列，比如随机数

## 导致执行不一样，有哪些？
1. 大多数指令在主备上执行结果相同：只要内存、寄存器一致，可以假设归纳结果是一样的
2. 外部输入（论文假设止只通过网络发packets）
    一般出现在中断时的DMA处理
3. 中断的时间
4. 不是状态功能的指令，例如读取当前时间。
5. 论文只研究单核

## 为什么执行不一样会有灾难的后果？
主备切换时，clients会看到不一致
比如：GFS的租约（lease）过期
1. 假设用GFS的master做主备
2. chunckserver必须要在60s的租约到期时，重新请求master续约
3. 如果chunckserver在极度快过期时发送续约
4. 对于primary，请求到达后，发生中断：拷贝新的租约，给相同的chunkserver
5. 对于backup，请求到达前，发生中断：拷贝旧的租约（已过期的）
6. 如果发生了主备切换，租约已经过期了，可能给其他chunkserver租约，
那么同一个chunk就有两台chunkserver拿到租约

所以，backup也要看到同样的事件，同样的事件，在指令流的同一个点

每个log entry：指令序号#， type， data

## FT处理时间中断
目标：主备要在在指令流的同一个点，看到中断

primary：
1. FT感知到中断
2. FT读取CPU的指令的序号
3. 发送"在指令X发生了中断"给backup
4. 产生中断给primary（这也是假设了primary的CPU在执行X发生了中断）

backup：
1. 忽略自身的时间硬件
2. FT看到上述的log entry在指令X之前！！
3. FT告诉CPU中断在指令X发生
4. FT模仿时间中断给backup

## FT处理网络包的到达（即输入）
primary：
1. FT通过NIC拷贝包，到FT的私有"bounce buffer"
2. 在NIC做DMA后，产生中断
3. FT获得中断
4. FT暂停primary
5. FT拷贝bounce buffer到primary的内存
6. FT模拟NIC的中断给primary
7. FT给发备份，指令#和包数据

backup
1. FT获得指令#的数据
2. 告诉CPU在指令X产生中断
3. FT拷贝数据进backup的内存，模拟NIC的中断

## 为什么用bounce buffer？
为了数据出现在primary和backup相同的执行点，防止出现不一致

## 注意backup必须滞后一个多个log entry
- 假设primary发生中断或者输入，在X之后
- 如果backup已经执行指令X，就无法正确处理输入
- 因此backup不能执行所有的指令，直到看到the first log entry
只执行到log entry的指令#，等待下一个log entry

## 例子：非确定的指令
即使主备状态相同，有些指令的结果依然不同，比如读当前数据、处理器序列号、cycle count

primary：
1. FT设置CPU中断
2. FT记录执行结果
3. 发生结果和指令#给backup

backup：
1. FT读取log entry，在指令#设置中断
2. FT提供primary算出的结果

## 关于输出（发送网络包）？
1. 主备都执行了输出指令
2. primary的FT执行了输出
3. backup的FT扔掉了输出

## 输出例子：DB服务
clients发生"increment"请求，DB做了increment存储了记录，并将新值响应

假设初始值是10，client发送了请求到primary，primary发送给backup，
然后两个FT都给了主/备，primary执行后结果是11，响应11，FT发送给client，
backup执行后结果是11，响应11，FT扔掉了响应。client得到了一次11的响应，这就对了

关于primary是否等待backup接收成功：
假设primary在响应后，挂了（因此client拿到了11），
并且logging channel还没发出去，此时backup goes live，它的内存值是10，
所以client再来请求，会得到11，而不是12。

解决：输出规则：
primary发出输出时，必须等待backup成功收到之前的所有log entries

使用输出规则后，

primary：

收到client 的请求，发生给backup，在响应给client值11前，
等待backup确认收到所有之前的log entry，在发送11给client。

1. 假设确认前，primary挂了，那么backup可能收到或没收到，
但至少primary没有响应给client
2. 假设确认后，primary挂了，那么client收到11了，
但backup已经拿到log entry，此时也能把值更新到11

## 输出规则是个重要问题
所有复制的系统都存在类似的问题形式，对性能有一定约束，
通过应用的特性逃避问题：比如操作是只读的，
FT是非应用层级的，必须是保守的，考虑最坏情况


Q：要是primary收到backup的确认后挂了，但还没响应client，怎么保证输出？
A：primary挂了，backup goes live会发生什么？backup收到log entry，继续执行，
输出依然被FT扔掉，最后一个log entry到达，停止扔掉。最后一个也就是client的请求，
将把输出响应client。

Q：要是primary在响应client后挂了？backup会在响应client第二遍？
A：是的，对于TCP请求是ok的，重复的seq会被忽略。磁盘写请求也ok，因为数据相同

主备切换时，重复的输出是复制系统中普遍存在的问题。
client需要有足够的状态忽略掉重复的响应或者设计为重复无害（duplicates are harmless）

Q：FT可以应对网络分裂吗？会产生脑裂吗？
比如，要是主备都觉得对方挂的情况，会怎样go live？
A：通过磁盘服务解决。磁盘服务提供test-and-set服务
如果主/备认为对方挂了，尝试test-and-set。
如果一个存活，那么test-and-set会执行成功，进而go live。
如果两个都尝试test-and-set，有一个将失败，然后halt。

磁盘服务是单点故障问题：要是磁盘服务挂了，服务就挂了。可能vm的人有备份磁盘服务

test-and-set服务初始化一个flag=false，将flag设为true，则go live。

Q：为什么不支持多核？

## 性能

![性能](/pic/202007/2020-07-19-19.22.06.png)

FT/Non-FT的比值：非常棒！仅一点慢

Logging bandwidth：直接反应磁盘读率（read rate）+网络输入率（input rate）
my-sqlL：18Mbit/s

有些数值低了：应用读取磁盘至少400Mbit/s，所以他们的应用不是io密集型的

## FT在何时更有吸引力？

1. 重要但低强度的服务，比如name server。
2. 软件不方便修改的服务。

## 关于高吞吐量的服务呢？
人们用应用级的复制状态机，比如数据库。状态值包含DB的，而非内存和磁盘。
而且事件，DB的指令（put、get），不会发包和中断。

结果：更细粒度的同步、更小的负载

GFS用的应用级的复制，Lab 2

## 总结：
主备复制
    VM-FT：清晰的案例
    
不带单点故障的分区涉及，如何处理？
    下一节
    
如何获得更好的性能？
    应用级的复制状态机






















